{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 从Hub模型中心加载数据   \n",
    "下例为MRPC中的glue数据集的加载过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({#训练集\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 3668\n",
    "    })\n",
    "    validation: Dataset({#验证集\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 408\n",
    "    })\n",
    "    test: Dataset({#测试集\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 1725\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到每个数据集包含train set, validation set, test set。每个集合包含四个列：     \n",
    "sentence1, sentence2, lable, idx    \n",
    "详细含义如下：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"sentence1\": Value(dtype=\"string\", id=None),\n",
    "    \"sentence2\": Value(dtype=\"string\", id=None),\n",
    "    \"label\": ClassLabel(\n",
    "        num_classes=2, names=[\"not_equivalent\", \"equivalent\"], names_file=None, id=None\n",
    "    ),\n",
    "    \"idx\": Value(dtype=\"int32\", id=None),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以采用字典访问每个集合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]\n",
    "\n",
    "\n",
    "{\n",
    "    \"idx\": 0,\n",
    "    \"label\": 1,\n",
    "    \"sentence1\": 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
    "    \"sentence2\": 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizer的token_type_ids      \n",
    "当给tokenizer传入句子大于1个时，需要token_type_ids来区分得到的inputid是第几句，例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs\n",
    "\n",
    "{\n",
    "    \"input_ids\": [\n",
    "        101,\n",
    "        2023,\n",
    "        2003,\n",
    "        1996,\n",
    "        2034,\n",
    "        6251,\n",
    "        1012,\n",
    "        102,\n",
    "        2023,\n",
    "        2003,\n",
    "        1996,\n",
    "        2117,\n",
    "        2028,\n",
    "        1012,\n",
    "        102,\n",
    "    ],\n",
    "    \"token_type_ids\": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "[\n",
    "    \"[CLS]\",\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"first\",\n",
    "    \"sentence\",\n",
    "    \".\",\n",
    "    \"[SEP]\",\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"second\",\n",
    "    \"one\",\n",
    "    \".\",\n",
    "    \"[SEP]\",\n",
    "]\n",
    "[\n",
    "    \"[CLS]\",\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"first\",\n",
    "    \"sentence\",\n",
    "    \".\",\n",
    "    \"[SEP]\",\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"second\",\n",
    "    \"one\",\n",
    "    \".\",\n",
    "    \"[SEP]\",\n",
    "]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若直接按照tokenizer格式（如下），最终得到的是一个字典，键是input_ids, attention_mask, token_type_ids，其值是对应值的列表，这样要求在运行中需要有足够内存存储整个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")#返回字典，对内存要求高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是dataset中的数据集是按照Apache Arrow格式存储，在运行时只需要运行接下来要用的数据，而不用加载全部数据集。   \n",
    "可以使用Dataset.map()方法将数据保存为dataset格式，具体方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #注意这里的tokenizer省略了padding参数，因为我们只需要填充到一个batch中最长句的大小即可\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
    "        num_rows: 3668\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
    "        num_rows: 408\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
    "        num_rows: 1725\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datesets库处理方式是对每个集合添加新的字段，每个字段对应预处理函数返回字典中的键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 动态填充   \n",
    "即将所有实例填充到该batch中最长元素的长度     \n",
    "可以使用tansformers的DataCollatorWithPadding函数完成，其需要传入tokenizer参数，以确定应该用什么token进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行该新函数的填充效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]#取训练集的前8条\n",
    "#删除列 idx ， sentence1 和 sentence2 ，因为不需要它们，而且删除包含字符串的列（我们不能用字符串创建张量）\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "#输出对应input_ids的长度\n",
    "[len(x) for x in samples[\"input_ids\"]]\n",
    "\n",
    "[50, 59, 47, 67, 59, 50, 62, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查data_collator是否进行动态填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "{\n",
    "    \"attention_mask\": torch.Size([8, 67]),\n",
    "    \"input_ids\": torch.Size([8, 67]),#包含8个样本，每个长度为67\n",
    "    \"token_type_ids\": torch.Size([8, 67]),\n",
    "    \"labels\": torch.Size([8]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用TrainerAPI微调模型    \n",
    "下例是以上所学对数据预处理的全部代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers提供了一个Trainer类，用来帮助微调过程，其难点在于准备Trainer.train()的环境，需要在GPU上运行，可以使用google Colab（需翻墙）     \n",
    "以下是定义Trainer类的工作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "#TrainingArguments包含训练和评估中的参数，我们只需要给他传入用于保存训练后的模型以及训练中的checkpoin目录\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "#定义model，这里可能有警告，因为Bert在没有在句子分类中进行过预训练，故对应的head被丢弃，警告一些权重没有使用，有些权重被随机初始化\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,#默认为DataCollatorWithPadding\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()#进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在微调中，每500步告诉一次损失。但不会告诉模型的质量如何，因为：     \n",
    "我们没有告诉他进行评估。比如将 evaluation_strategy 设置为“ step ”（在每个 eval_steps 步骤评估一次）或“ epoch ”（在每个 epoch 结束时评估）。     \n",
    "我们没有为 Trainer 提供一个 compute_metrics() 函数来计算上述评估过程的指标（否则评估将只会输出 loss，但这不是一个非常直观的数字）。     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 评估\n",
    "如上所述，需要构建compute_metrics()函数，该函数必须接收一个 EvalPrediction 对象（它是一个带有 predictions 和 label_ids 字段的参数元组），并将返回一个字符串映射到浮点数的字典（字符串是返回的指标名称，而浮点数是其值）   \n",
    "接下来是Trainer.predict()的过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "#predictions.predictions表示预测结果，predictions.label_ids表示在验证集中的真实标签\n",
    "(408, 2) (408,)\n",
    "#第一个表示predictions.predictions有408个样本，2表示两个类别的概率（是logits形式）\n",
    "#第二个是真实标签，有408个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict() 方法的输出另一个带有三个字段的命名元组: predictions label_ids 和 metrics metrics 字段将只包含所传递的数据集的损失,以及一些时间指标(总共花费的时间和平均预测时间)。   \n",
    "若我们自己定义了compute_metrics函数，并将其传给Trainer，该字段还包括compute_metrics的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面代码的predictions.predictions中的logits形式通过softmax转化为概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以将preds与标签进行比对。    \n",
    "compute_metrics可以使用Evaluate库中的指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "\n",
    "#结果可能与此不同，因为正如我们上面所言，bert没有对分类任务进行预训练，所以权重是随机的\n",
    "{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有打包，就是compute_metrics函数，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "#将其加入Trainer类，完整过程如下：\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "#注意采用新的TrainingArguments,否则将会在旧的已训练过的模型上继续训练\n",
    "#epoch表示在每个epoch结束时报告损失和指标\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 完整训练（不实用Trainer方法，实现上述步骤）（基于Pytorch）      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练前准备   \n",
    "删除不需要的列，如sentence1和sentence2   \n",
    "将列名label改为labels（pytorch默认输入为labels）     \n",
    "设置格式，使其返回Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "\n",
    "\n",
    "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#至此，可轻松定义数据加载器     \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    #shuffle表示是否对数据集进行随机打乱\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证数据处理有无错误\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "#可能会有所不同，因为shuffle=true，且会自动填充到batch中最大长度\n",
    "{'attention_mask': torch.Size([8, 65]),\n",
    " 'input_ids': torch.Size([8, 65]),\n",
    " 'labels': torch.Size([8]),\n",
    " 'token_type_ids': torch.Size([8, 65])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行模型实例化\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "#测试，传入batch\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "\n",
    "\n",
    "\n",
    "tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练循环\n",
    "但仍缺少优化器和学习率调度器    \n",
    "Trainer默认的优化器是AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率调度器的定义，需要训练次数，即训练次数（epochs）*batch数量    \n",
    "Trainer一般默认3个epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    #这个参数指定了在训练开始时，学习率 线性增加 的步数，直到达到最终的学习率。通常用于优化器的学习率调度，特别是在使用学习率预热（warmup）策略时。\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率预热，指让学习率在num_warmup_steps内线性增加到初始学习率，上例中将其设为0，即表示学习率直接从初始学习率开始。    \n",
    "其作用可以用来避免梯度爆炸或更新步伐较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#将训练转向gpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "device(type='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以开始训练，但为了方便观察训练次数，使用tqdm库，它可以将循环可视化，显示类似于一个进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()#转入训练模式\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}#将数据转入gpu\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()#计算梯度\n",
    "\n",
    "        optimizer.step()#优化器进行更新\n",
    "        lr_scheduler.step()#更新学习率\n",
    "        optimizer.zero_grad()#清除梯度，pytorch会在反向传播中自动积累梯度\n",
    "        progress_bar.update(1)#进度条更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码仍缺少一部分，即需要在每层循环中告知当前模型状态，故引入评估循环     \n",
    "如上所述，利用Evaluate库引入compute_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()#转入评估模式\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():#禁用梯度，因此时只是在评估，不需要计算梯度，可节省时间\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n",
    "\n",
    "\n",
    "{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上述过程总结（没有评估）\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述基础上，引入Accelerate库，从而实现在多块gpu上分布训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ from accelerate import Accelerator\n",
    "  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "+ accelerator = Accelerator()\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "#删减\n",
    "- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "- model.to(device)\n",
    "\n",
    "+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "+     train_dataloader, eval_dataloader, model, optimizer\n",
    "+ )\n",
    "\n",
    "  num_epochs = 3\n",
    "  num_training_steps = num_epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "      for batch in train_dataloader:\n",
    "-         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "-         loss.backward()\n",
    "+         accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用Accelerate完整代码\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  关于其accelerate的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "！accelerate config\n",
    "！accelerate launch train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#若是在notebook中\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

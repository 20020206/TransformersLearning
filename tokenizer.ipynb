{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline包含三个步骤：   \n",
    "预处理、模型计算和后处理  \n",
    "预处理：将文本转化为inputID，由Tokenizer完成  \n",
    "模型计算：利用对应机器学习及深度学习算法，得到结果数字，通常为logit（对数函数）  \n",
    "后处理：将logits转化为概率即prediction  \n",
    "\n",
    "\n",
    "利用tokenizer获取input  \n",
    "tokenizer负责：将对应单词转换为token，再将token映射到一个数字，称为input ID  \n",
    "然后将其转化为tensor  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization：通过tokenizer的tokenize()方法实现，该方法输出为字符表，即tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token向inputID的转化：通过tokenizer的convert_token_to_ids()方法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)\n",
    "\n",
    "\n",
    "\n",
    "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述输出InputID若是有适当的框架或size，即可作为tensor成为模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)\n",
    "\n",
    "'Using a Transformer network is simple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还需注意，作为模型输入的tensor在单纯的inputID上加了一层维度   \n",
    "例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# 这一行会运行失败，原因在于通过convert_tokens_to_ids得到的还差一层维度才能直接作为tensor被输入模型\n",
    "model(input_ids)\n",
    "\n",
    "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])\n",
    "#此处直接实行tokenizer函数，相当于自动完成tokenize和convert过程，且自动加了一层维度，所以是合格的tensor\n",
    "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "          2607,  2026,  2878,  2166,  1012,   102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#对上述错误代码修改如下，其实就是加上一层维度\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "\n",
    "\n",
    "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
    "Logits: [[-2.7276,  2.8789]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 填充输入(padding)         \n",
    "若是进行批处理(batch)，会出现两个句子不一样上（单词量多少不一样），从而inputID并不是一个矩阵，无法作为tensor，所以需要对较短的句子对应的INputID进行填充。       \n",
    "但若单纯只使用padding,会发生以下问题：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)\n",
    "\n",
    "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会发现：加了padding，会导致model过后的logits值发生改变，这是因为Transformers的注意力层特性，也就是说，Transformers会注意上下文，而pad也会被纳入该考虑范围。    \n",
    "##### 为了解决该问题，引入attention mask，即标记哪些token该被忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizer函数，包含tokenize以及convert操作，十分强大，还具有填充、截断的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子序列填充到最长句子的长度\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# 将句子序列填充到模型的最大长度\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# 将句子序列填充到指定的最大长度\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# 将截断比模型最大长度长的句子序列\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# 将截断长于指定最大长度的句子序列\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# 返回 PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# 返回 TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# 返回 NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer还会加上特殊的token即[CLS]和[SEP]，分别用来标记开始和结束"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
